# MLPerf Storage benchmark sample submission

## What to submit - CLOSED submissions
A complete submission for one benchmark (3D-Unet and/or BERT) contains 3 folders:
 
#### results folder containing (for each system):
The entire output folder generated by running MLPerf Storage benchmark. 
Final submission JSON summary file `mlperf_storage_report.json` must be present in the results folder. The JSON file must be generated using ./benchmark.sh reportgen script.
Structure the output as shown in this example: https://github.com/johnugeorge/mlperf-storage-sample-results 

#### systems folder containing
    <system-name>.json
    <system-name>.pdf

The purpose of the system files is to provide sufficient detail on the system to enable full reproduction by a third party. The goal of the pdf file is to complement the JSON file, providing additional system details with diagrams and more description.

For system naming examples look [here](https://github.com/mlcommons/inference_results_v3.0/tree/main/closed)

### code folder containing:
Source code of the [MLPerf Storage benchmark](https://github.com/mlcommons/storage/tree/v0.5-branch). 

## What to submit - OPEN submissions
Everything that is required for a CLOSED submission, following the same structure.
Additional source code used for the OPEN Submission benchmark implementations must be available under a license that permits MLCommons to use the implementation for benchmarking. 
